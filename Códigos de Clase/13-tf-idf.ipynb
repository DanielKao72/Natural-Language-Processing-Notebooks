{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd97cde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\danik\\documents\\repositorios\\natural-language-processing-notebooks\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\danik\\documents\\repositorios\\natural-language-processing-notebooks\\.venv\\lib\\site-packages (from scikit-learn) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\danik\\documents\\repositorios\\natural-language-processing-notebooks\\.venv\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\danik\\documents\\repositorios\\natural-language-processing-notebooks\\.venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\danik\\documents\\repositorios\\natural-language-processing-notebooks\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c9f1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Successfully uninstalled numpy-1.26.4\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.4-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Downloading numpy-2.3.4-cp313-cp313-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.8 MB 1.6 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.8/12.8 MB 1.7 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.3/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.6/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.1/12.8 MB 1.9 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.6/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 3.1/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.7/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.2/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.7/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.2/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.8/12.8 MB 2.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 6.0/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.3/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.8/12.8 MB 2.1 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 7.3/12.8 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.9/12.8 MB 2.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.7/12.8 MB 2.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 9.2/12.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.7/12.8 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 10.2/12.8 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.7/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.3/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 2.3 MB/s  0:00:05\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.3.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "node2vec 0.5.0 requires numpy<2.0.0,>=1.24.0, but you have numpy 2.3.4 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy -y\n",
    "!pip install numpy --upgrade --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af9da183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7501993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 7)\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 20 stored elements and shape (4, 7)>\n",
      "  Coords\tValues\n",
      "  (0, 3)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 0)\t2\n",
      "  (1, 5)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 6)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"Este es el primer documento.\",\n",
    "    \"Este documento es el segundo documento.\",\n",
    "    \"Y este es el tercer documento.\",\n",
    "    \"Â¿Es este el primer documento?\",\n",
    "]\n",
    "\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed47d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['documento' 'el' 'es' 'este' 'primer' 'segundo' 'tercer']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b46da2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 0 0]\n",
      " [2 1 1 1 0 1 0]\n",
      " [1 1 1 1 0 0 1]\n",
      " [1 1 1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9686caca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'este': 3, 'es': 2, 'el': 1, 'primer': 4, 'documento': 0, 'segundo': 5, 'tercer': 6}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b912c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f488d5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 19)\n",
      "['documento' 'documento es' 'el' 'el primer' 'el segundo' 'el tercer' 'es'\n",
      " 'es el' 'es este' 'este' 'este documento' 'este el' 'este es' 'primer'\n",
      " 'primer documento' 'segundo' 'segundo documento' 'tercer'\n",
      " 'tercer documento']\n",
      "[[1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0]\n",
      " [2 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0]\n",
      " [1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1]\n",
      " [1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "x_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "print(x_2.shape)\n",
    "print(bigram_vectorizer.get_feature_names_out())\n",
    "print(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6769d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 30)\n",
      "['documento' 'documento es' 'documento es el' 'el' 'el primer'\n",
      " 'el primer documento' 'el segundo' 'el segundo documento' 'el tercer'\n",
      " 'el tercer documento' 'es' 'es el' 'es el primer' 'es el segundo'\n",
      " 'es el tercer' 'es este' 'es este el' 'este' 'este documento'\n",
      " 'este documento es' 'este el' 'este el primer' 'este es' 'este es el'\n",
      " 'primer' 'primer documento' 'segundo' 'segundo documento' 'tercer'\n",
      " 'tercer documento']\n",
      "[[1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [2 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1]\n",
      " [1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Trigaramas\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 3),min_df=1)\n",
    "x_3 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "print(x_3.shape)\n",
    "print(bigram_vectorizer.get_feature_names_out())\n",
    "print(x_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f13577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 39)\n",
      "['documento' 'documento es' 'documento es el' 'documento es el segundo'\n",
      " 'el' 'el primer' 'el primer documento' 'el segundo'\n",
      " 'el segundo documento' 'el tercer' 'el tercer documento' 'es' 'es el'\n",
      " 'es el primer' 'es el primer documento' 'es el segundo'\n",
      " 'es el segundo documento' 'es el tercer' 'es el tercer documento'\n",
      " 'es este' 'es este el' 'es este el primer' 'este' 'este documento'\n",
      " 'este documento es' 'este documento es el' 'este el' 'este el primer'\n",
      " 'este el primer documento' 'este es' 'este es el' 'este es el primer'\n",
      " 'este es el tercer' 'primer' 'primer documento' 'segundo'\n",
      " 'segundo documento' 'tercer' 'tercer documento']\n",
      "[[1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0\n",
      "  0 0 0]\n",
      " [2 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1\n",
      "  1 0 0]\n",
      " [1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
      "  0 1 1]\n",
      " [1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0\n",
      "  0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Tetragramas\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 4),min_df=1)\n",
    "x_4 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "print(x_4.shape)\n",
    "print(bigram_vectorizer.get_feature_names_out())\n",
    "print(x_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44dd678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 7)\n",
      "['documento' 'el' 'es' 'este' 'primer' 'segundo' 'tercer']\n",
      "[[0.39896105 0.39896105 0.39896105 0.39896105 0.60276058 0.\n",
      "  0.        ]\n",
      " [0.61221452 0.30610726 0.30610726 0.30610726 0.         0.5865905\n",
      "  0.        ]\n",
      " [0.361028   0.361028   0.361028   0.361028   0.         0.\n",
      "  0.69183461]\n",
      " [0.39896105 0.39896105 0.39896105 0.39896105 0.60276058 0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "x_train_tfidf = tfidf_vect.fit_transform(corpus)\n",
    "print(x_train_tfidf.shape)\n",
    "print(tfidf_vect.get_feature_names_out())\n",
    "print(x_train_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37dd5520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 30)\n",
      "['documento' 'documento es' 'documento es el' 'el' 'el primer'\n",
      " 'el primer documento' 'el segundo' 'el segundo documento' 'el tercer'\n",
      " 'el tercer documento' 'es' 'es el' 'es el primer' 'es el segundo'\n",
      " 'es el tercer' 'es este' 'es este el' 'este' 'este documento'\n",
      " 'este documento es' 'este el' 'este el primer' 'este es' 'este es el'\n",
      " 'primer' 'primer documento' 'segundo' 'segundo documento' 'tercer'\n",
      " 'tercer documento']\n",
      "[[0.20913449 0.         0.         0.20913449 0.31596574 0.31596574\n",
      "  0.         0.         0.         0.         0.20913449 0.2558015\n",
      "  0.40076248 0.         0.         0.         0.         0.20913449\n",
      "  0.         0.         0.         0.         0.31596574 0.31596574\n",
      "  0.31596574 0.31596574 0.         0.         0.         0.        ]\n",
      " [0.31028974 0.29730268 0.29730268 0.15514487 0.         0.\n",
      "  0.29730268 0.29730268 0.         0.         0.15514487 0.18976445\n",
      "  0.         0.29730268 0.         0.         0.         0.15514487\n",
      "  0.29730268 0.29730268 0.         0.         0.         0.\n",
      "  0.         0.         0.29730268 0.29730268 0.         0.        ]\n",
      " [0.18757363 0.         0.         0.18757363 0.         0.\n",
      "  0.         0.         0.3594456  0.3594456  0.18757363 0.22942947\n",
      "  0.         0.         0.3594456  0.         0.         0.18757363\n",
      "  0.         0.         0.         0.         0.28339104 0.28339104\n",
      "  0.         0.         0.         0.         0.3594456  0.3594456 ]\n",
      " [0.18959584 0.         0.         0.18959584 0.28644625 0.28644625\n",
      "  0.         0.         0.         0.         0.18959584 0.\n",
      "  0.         0.         0.         0.36332075 0.36332075 0.18959584\n",
      "  0.         0.         0.36332075 0.36332075 0.         0.\n",
      "  0.28644625 0.28644625 0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Trigaramas TF-IDF\n",
    "tfidf_vect_3 = TfidfVectorizer(ngram_range=(1, 3))\n",
    "x_train_tfidf_3 = tfidf_vect_3.fit_transform(corpus)\n",
    "print(x_train_tfidf_3.shape)\n",
    "print(tfidf_vect_3.get_feature_names_out())\n",
    "print(x_train_tfidf_3.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76ed220b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 39)\n",
      "['documento' 'documento es' 'documento es el' 'documento es el segundo'\n",
      " 'el' 'el primer' 'el primer documento' 'el segundo'\n",
      " 'el segundo documento' 'el tercer' 'el tercer documento' 'es' 'es el'\n",
      " 'es el primer' 'es el primer documento' 'es el segundo'\n",
      " 'es el segundo documento' 'es el tercer' 'es el tercer documento'\n",
      " 'es este' 'es este el' 'es este el primer' 'este' 'este documento'\n",
      " 'este documento es' 'este documento es el' 'este el' 'este el primer'\n",
      " 'este el primer documento' 'este es' 'este es el' 'este es el primer'\n",
      " 'este es el tercer' 'primer' 'primer documento' 'segundo'\n",
      " 'segundo documento' 'tercer' 'tercer documento']\n",
      "[[0.18194407 0.         0.         0.         0.18194407 0.27488576\n",
      "  0.27488576 0.         0.         0.         0.         0.18194407\n",
      "  0.22254371 0.34865773 0.34865773 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.18194407 0.\n",
      "  0.         0.         0.         0.         0.         0.27488576\n",
      "  0.27488576 0.34865773 0.         0.27488576 0.27488576 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.27586306 0.26431691 0.26431691 0.26431691 0.13793153 0.\n",
      "  0.         0.26431691 0.26431691 0.         0.         0.13793153\n",
      "  0.16871006 0.         0.         0.26431691 0.26431691 0.\n",
      "  0.         0.         0.         0.         0.13793153 0.26431691\n",
      "  0.26431691 0.26431691 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26431691\n",
      "  0.26431691 0.         0.        ]\n",
      " [0.16720991 0.         0.         0.         0.16720991 0.\n",
      "  0.         0.         0.         0.32042281 0.32042281 0.16720991\n",
      "  0.20452173 0.         0.         0.         0.         0.32042281\n",
      "  0.32042281 0.         0.         0.         0.16720991 0.\n",
      "  0.         0.         0.         0.         0.         0.25262502\n",
      "  0.25262502 0.         0.32042281 0.         0.         0.\n",
      "  0.         0.32042281 0.32042281]\n",
      " [0.16863767 0.         0.         0.         0.16863767 0.25478211\n",
      "  0.25478211 0.         0.         0.         0.         0.16863767\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.3231588  0.3231588  0.3231588  0.16863767 0.\n",
      "  0.         0.         0.3231588  0.3231588  0.3231588  0.\n",
      "  0.         0.         0.         0.25478211 0.25478211 0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Tetragaramas TF-IDF\n",
    "tfidf_vect_3 = TfidfVectorizer(ngram_range=(1, 4))\n",
    "x_train_tfidf_3 = tfidf_vect_3.fit_transform(corpus)\n",
    "print(x_train_tfidf_3.shape)\n",
    "print(tfidf_vect_3.get_feature_names_out())\n",
    "print(x_train_tfidf_3.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
